{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "landing-page-gan.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sajjad-shirazy/python-notebooks/blob/master/landing_page_gan.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GGElWj6VsiIt",
        "colab_type": "text"
      },
      "source": [
        "# Landing page GAN (Generative Adversarial Networks)\n",
        "Sajjad Shirazy @ Stijlbreuk - 2019\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EAntePt6sZjm",
        "colab_type": "text"
      },
      "source": [
        "## Uploading data set from Google drive into the VM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EG4hMblcsN6C",
        "colab_type": "code",
        "outputId": "09dc11bd-d575-4608-8b9b-8d5b40c585e4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        }
      },
      "source": [
        "# mounting google drive into this VM\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "print('mounted ...');\n",
        "\n",
        "# unzipping the target\n",
        "import zipfile\n",
        "\n",
        "zip_ref = zipfile.ZipFile('/content/gdrive/My Drive/ML/1000-landing-pages.zip', 'r')\n",
        "zip_ref.extractall('/data/1000-landing-pages')\n",
        "zip_ref.close();\n",
        "print('unzipped ...');\n",
        "print('done');"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n",
            "mounted ...\n",
            "unzipped ...\n",
            "done\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bVMtINphuDIY",
        "colab_type": "code",
        "outputId": "2c092661-beba-47bc-f816-ed2c6b127606",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        }
      },
      "source": [
        "# check if every thing is ok\n",
        "!ls /data/1000-landing-pages/ -U | head -4"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "5cac70f1194ca553458634.jpg\n",
            "5b0c017f8e74b.jpg\n",
            "586b69c3ea20f.jpg\n",
            "5cd2a04a1d317040955559.png\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bnPN0Biqx5tL",
        "colab_type": "text"
      },
      "source": [
        "## Importing packages\n",
        "\n",
        "Let's start by importing required packages:\n",
        "\n",
        "*   os — to read files and directory structure\n",
        "*   numpy — for some matrix math outside of TensorFlow\n",
        "*   matplotlib.pyplot — to plot the graph and display images in our training and validation data\n",
        "\n",
        "For the TensorFlow imports, we directly specify Keras symbols (Sequential, Dense, etc.). This enables us to refer to these names directly in our code without having to qualify their full names (for example, `Dense` instead of `tf.keras.layer.Dense`).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6sM7edBLN0T5",
        "colab_type": "code",
        "outputId": "38e594ac-bc76-4cd5-c1a4-6af38b80ba23",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 540
        }
      },
      "source": [
        "!pip install tensorflow-gpu==2.0.0-alpha0"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow-gpu==2.0.0-alpha0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1a/66/32cffad095253219d53f6b6c2a436637bbe45ac4e7be0244557210dc3918/tensorflow_gpu-2.0.0a0-cp36-cp36m-manylinux1_x86_64.whl (332.1MB)\n",
            "\u001b[K     |████████████████████████████████| 332.1MB 33kB/s \n",
            "\u001b[?25hRequirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-alpha0) (1.0.7)\n",
            "Collecting tf-estimator-nightly<1.14.0.dev2019030116,>=1.14.0.dev2019030115 (from tensorflow-gpu==2.0.0-alpha0)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/13/82/f16063b4eed210dc2ab057930ac1da4fbe1e91b7b051a6c8370b401e6ae7/tf_estimator_nightly-1.14.0.dev2019030115-py2.py3-none-any.whl (411kB)\n",
            "\u001b[K     |████████████████████████████████| 419kB 38.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-alpha0) (0.33.1)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-alpha0) (1.1.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-alpha0) (1.0.9)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-alpha0) (1.15.0)\n",
            "Requirement already satisfied: numpy<2.0,>=1.14.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-alpha0) (1.16.3)\n",
            "Collecting tb-nightly<1.14.0a20190302,>=1.14.0a20190301 (from tensorflow-gpu==2.0.0-alpha0)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a9/51/aa1d756644bf4624c03844115e4ac4058eff77acd786b26315f051a4b195/tb_nightly-1.14.0a20190301-py3-none-any.whl (3.0MB)\n",
            "\u001b[K     |████████████████████████████████| 3.0MB 19.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-alpha0) (0.2.2)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-alpha0) (1.12.0)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-alpha0) (0.7.1)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-alpha0) (0.7.1)\n",
            "Collecting google-pasta>=0.1.2 (from tensorflow-gpu==2.0.0-alpha0)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f9/68/a14620bfb042691f532dcde8576ff82ee82e4c003cdc0a3dbee5f289cee6/google_pasta-0.1.6-py3-none-any.whl (51kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 23.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-alpha0) (3.7.1)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.6->tensorflow-gpu==2.0.0-alpha0) (2.8.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<1.14.0a20190302,>=1.14.0a20190301->tensorflow-gpu==2.0.0-alpha0) (0.15.2)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<1.14.0a20190302,>=1.14.0a20190301->tensorflow-gpu==2.0.0-alpha0) (3.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tensorflow-gpu==2.0.0-alpha0) (41.0.1)\n",
            "Installing collected packages: tf-estimator-nightly, tb-nightly, google-pasta, tensorflow-gpu\n",
            "Successfully installed google-pasta-0.1.6 tb-nightly-1.14.0a20190301 tensorflow-gpu-2.0.0a0 tf-estimator-nightly-1.14.0.dev2019030115\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z6SeZrCavisf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras import layers\n",
        "# tf.logging.set_verbosity(tf.logging.ERROR)\n",
        "from tensorflow.keras.layers import Input, Dense, Reshape, Flatten, Dropout\n",
        "from tensorflow.keras.layers import BatchNormalization, Activation, ZeroPadding2D\n",
        "from tensorflow.keras.layers import LeakyReLU\n",
        "from tensorflow.keras.layers import UpSampling2D, Conv2D\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.optimizers import Adam"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wQm5BDZ5xsU5",
        "colab_type": "text"
      },
      "source": [
        "## Setting Model Parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oYTkI3Y3xwn_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BATCH_SIZE = 64  # Number of training examples to process before updating our models variables\n",
        "IMG_SHAPE_W  = 28  # Our training data consists of images with width of 320 pixels\n",
        "IMG_SHAPE_H  = 28  # Our training data consists of images with height of 240 pixels\n",
        "IMG_SHAPE_C  = 3  # CHANELS\n",
        "latent_dim = 100;\n",
        "img_shape = (IMG_SHAPE_W, IMG_SHAPE_H, IMG_SHAPE_C)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y-WRtWc9vdPw",
        "colab_type": "text"
      },
      "source": [
        "## Data Preparation\n",
        "Images must be formatted into appropriately pre-processed floating point tensors before being fed into the network. The steps involved in preparing these images are:\n",
        "\n",
        "1. Read images from the disk\n",
        "2. Decode contents of these images and convert it into proper grid format as per their RGB content\n",
        "3. Convert them into floating point tensors\n",
        "4. Rescale the tensors from values between 0 and 255 to values between 0 and 1, as neural networks prefer to deal with small input values.\n",
        "\n",
        "Fortunately, all these tasks can be done using the class **tf.keras.preprocessing.image.ImageDataGenerator**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qd-zzxkMxmGD",
        "colab_type": "code",
        "outputId": "c7c52a5f-9a20-42b6-cf7b-057735973a2a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "train_image_generator = ImageDataGenerator(rescale=1./255)  # Generator for our training data\n",
        "train_data_gen = train_image_generator.flow_from_directory(\n",
        "    batch_size=BATCH_SIZE, \n",
        "    directory='/data/', \n",
        "    shuffle=True, \n",
        "    target_size=(IMG_SHAPE_W, IMG_SHAPE_H), #(150,150)\n",
        "    class_mode='binary'\n",
        ")"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 986 images belonging to 1 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yI3AuJanzhSm",
        "colab_type": "text"
      },
      "source": [
        "### Visualizing Training images\n",
        "We can visualize our training images by getting a batch of images from the training generator, and then plotting a few of them using matplotlib."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wtiwJRGezkbX",
        "colab_type": "code",
        "outputId": "6eff4268-93f9-4e36-b302-0b30df9a78f8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        }
      },
      "source": [
        "# This function will plot images in the form of a grid with 1 row and 5 columns where images are placed in each column.\n",
        "def plotImages(images_arr):\n",
        "    fig, axes = plt.subplots(1, 5, figsize=(20,20))\n",
        "    axes = axes.flatten()\n",
        "    for img, ax in zip( images_arr, axes):\n",
        "        ax.imshow(img)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "sample_training_images, _ = next(train_data_gen)     \n",
        "plotImages(sample_training_images[:5])  # Plot images 0-4"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAABZgAAAEeCAYAAAAHC3ASAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nfJMAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 1440x1440 with 5 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yWJaozA5zxf0",
        "colab_type": "text"
      },
      "source": [
        "The next function returns a batch from the dataset. One batch is a tuple of (many images, many labels). For right now, we're discarding the labels because we just want to look at the images."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-a8Kei58SLLs",
        "colab_type": "text"
      },
      "source": [
        "## Create the models\n",
        "\n",
        "Both the generator and discriminator are defined using the [Keras Sequential API](https://www.tensorflow.org/guide/keras#sequential_model)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q5W-mHcTSYv9",
        "colab_type": "text"
      },
      "source": [
        "### The Generator\n",
        "\n",
        "The generator uses `tf.keras.layers.Conv2DTranspose` (upsampling) layers to produce an image from a seed (random noise). Start with a `Dense` layer that takes this seed as input, then upsample several times until you reach the desired image size of 28x28x1. Notice the `tf.keras.layers.LeakyReLU` activation for each layer, except the output layer which uses tanh."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v1oBwVFpSKH5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def make_generator_model():\n",
        "    model = tf.keras.Sequential()\n",
        "#     model.add(layers.Dense(7*7*256, use_bias=False, input_shape=(100,)))\n",
        "#     model.add(layers.BatchNormalization())\n",
        "#     model.add(layers.LeakyReLU())\n",
        "\n",
        "#     model.add(layers.Reshape((7, 7, 256)))\n",
        "#     assert model.output_shape == (None, 7, 7, 256) # Note: None is the batch size\n",
        "\n",
        "#     model.add(layers.Conv2DTranspose(128, (5, 5), strides=(1, 1), padding='same', use_bias=False))\n",
        "#     assert model.output_shape == (None, 7, 7, 128)\n",
        "#     model.add(layers.BatchNormalization())\n",
        "#     model.add(layers.LeakyReLU())\n",
        "\n",
        "#     model.add(layers.Conv2DTranspose(64, (5, 5), strides=(2, 2), padding='same', use_bias=False))\n",
        "#     assert model.output_shape == (None, 14, 14, 64)\n",
        "#     model.add(layers.BatchNormalization())\n",
        "#     model.add(layers.LeakyReLU())\n",
        "\n",
        "#     model.add(layers.Conv2DTranspose(1, (5, 5), strides=(2, 2), padding='same', use_bias=False, activation='tanh'))\n",
        "#     assert model.output_shape == (None, IMG_SHAPE_W, IMG_SHAPE_H, 1)\n",
        "\n",
        "#     return model\n",
        "    model.add(Dense(256, input_dim=latent_dim))\n",
        "    model.add(LeakyReLU(alpha=0.2))\n",
        "    model.add(BatchNormalization(momentum=0.8))\n",
        "    model.add(Dense(512))\n",
        "    model.add(LeakyReLU(alpha=0.2))\n",
        "    model.add(BatchNormalization(momentum=0.8))\n",
        "    model.add(Dense(1024))\n",
        "    model.add(LeakyReLU(alpha=0.2))\n",
        "    model.add(BatchNormalization(momentum=0.8))\n",
        "    model.add(Dense(np.prod(img_shape), activation='tanh'))\n",
        "    model.add(Reshape(img_shape))\n",
        "    model.summary()\n",
        "    noise = Input(shape=(latent_dim,))\n",
        "    img = model(noise)\n",
        "    return Model(noise, img)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "GyWgG09LCSJl"
      },
      "source": [
        "Use the (as yet untrained) generator to create an image."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mNXn3lYzUicK",
        "colab_type": "code",
        "outputId": "df98abbf-967a-48a3-8127-4be18ab9004e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 807
        }
      },
      "source": [
        "generator = make_generator_model()\n",
        "\n",
        "noise = tf.random.normal([1, latent_dim])\n",
        "generated_image = generator(noise, training=False)\n",
        "\n",
        "plt.imshow(generated_image[0, :, :, 0])"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_7 (Dense)              (None, 256)               25856     \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_5 (LeakyReLU)    (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_v2_3 (Ba (None, 256)               1024      \n",
            "_________________________________________________________________\n",
            "dense_8 (Dense)              (None, 512)               131584    \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_6 (LeakyReLU)    (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_v2_4 (Ba (None, 512)               2048      \n",
            "_________________________________________________________________\n",
            "dense_9 (Dense)              (None, 1024)              525312    \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_7 (LeakyReLU)    (None, 1024)              0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_v2_5 (Ba (None, 1024)              4096      \n",
            "_________________________________________________________________\n",
            "dense_10 (Dense)             (None, 2352)              2410800   \n",
            "_________________________________________________________________\n",
            "reshape_1 (Reshape)          (None, 28, 28, 3)         0         \n",
            "=================================================================\n",
            "Total params: 3,100,720\n",
            "Trainable params: 3,097,136\n",
            "Non-trainable params: 3,584\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f3fbd3986d8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 52
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ndmo7Azh+AAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "D0IKnaCtg6WE"
      },
      "source": [
        "### The Discriminator\n",
        "\n",
        "The discriminator is a CNN-based image classifier."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wgOUGJCFrArv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def make_discriminator_model():\n",
        "#     model = tf.keras.Sequential()\n",
        "#     model.add(layers.Conv2D(64, (5, 5), strides=(2, 2), padding='same',\n",
        "#                                      input_shape=[28, 28, 1]))\n",
        "#     model.add(layers.LeakyReLU())\n",
        "#     model.add(layers.Dropout(0.3))\n",
        "\n",
        "#     model.add(layers.Conv2D(128, (5, 5), strides=(2, 2), padding='same'))\n",
        "#     model.add(layers.LeakyReLU())\n",
        "#     model.add(layers.Dropout(0.3))\n",
        "\n",
        "#     model.add(layers.Flatten())\n",
        "#     model.add(layers.Dense(1))\n",
        "#     return model\n",
        "    model = Sequential()\n",
        "\n",
        "    model.add(Flatten(input_shape=img_shape))\n",
        "    model.add(Dense(512))\n",
        "    model.add(LeakyReLU(alpha=0.2))\n",
        "    model.add(Dense(256))\n",
        "    model.add(LeakyReLU(alpha=0.2))\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "    model.summary()\n",
        "\n",
        "    img = Input(shape=img_shape)\n",
        "    validity = model(img)\n",
        "\n",
        "    return Model(img, validity)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "QhPneagzCaQv"
      },
      "source": [
        "Use the (as yet untrained) discriminator to classify the generated images as real or fake. The model will be trained to output positive values for real images, and negative values for fake images."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lLusTZDwrKaI",
        "colab_type": "code",
        "outputId": "0ac8d80a-0e64-4c8d-c42f-d5fcb254fd69",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 381
        }
      },
      "source": [
        "discriminator = make_discriminator_model()\n",
        "decision = discriminator(generated_image)\n",
        "print (decision)"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "flatten_1 (Flatten)          (None, 2352)              0         \n",
            "_________________________________________________________________\n",
            "dense_11 (Dense)             (None, 512)               1204736   \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_8 (LeakyReLU)    (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_12 (Dense)             (None, 256)               131328    \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_9 (LeakyReLU)    (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_13 (Dense)             (None, 1)                 257       \n",
            "=================================================================\n",
            "Total params: 1,336,321\n",
            "Trainable params: 1,336,321\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "tf.Tensor([[0.5608148]], shape=(1, 1), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ctoybJFYQxtY",
        "colab_type": "text"
      },
      "source": [
        "## Compiling models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QLVCbP1uQ2Qo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "optimizer = Adam(0.0002, 0.5)\n",
        "# Build and compile the discriminator\n",
        "discriminator.compile(loss='binary_crossentropy',\n",
        "    optimizer=optimizer,\n",
        "    metrics=['accuracy'])\n",
        "\n",
        "# The generator takes noise as input and generates imgs\n",
        "z = Input(shape=(latent_dim,))\n",
        "img = generator(z)\n",
        "\n",
        "# For the combined model we will only train the generator\n",
        "discriminator.trainable = False\n",
        "\n",
        "# The discriminator takes generated images as input and determines validity\n",
        "validity = discriminator(img)\n",
        "\n",
        "# The combined model  (stacked generator and discriminator)\n",
        "# Trains the generator to fool the discriminator\n",
        "combined = Model(z, validity)\n",
        "combined.compile(loss='binary_crossentropy', optimizer=optimizer)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ntXXfxikSnbG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(epochs, batch_size=128, sample_interval=50):\n",
        "\n",
        "        # Load the dataset\n",
        "#         (X_train, _), (_, _) = mnist.load_data()\n",
        "\n",
        "      # Rescale -1 to 1\n",
        "#         X_train = X_train / 127.5 - 1.\n",
        "#         X_train = np.expand_dims(X_train, axis=3)\n",
        "\n",
        "      # Adversarial ground truths\n",
        "    valid = np.ones((batch_size, 1))\n",
        "    fake = np.zeros((batch_size, 1))\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "\n",
        "          # ---------------------\n",
        "          #  Train Discriminator\n",
        "          # ---------------------\n",
        "\n",
        "            # Select a random batch of images\n",
        "  #             idx = np.random.randint(0, X_train.shape[0], batch_size)\n",
        "  #             imgs = X_train[idx]\n",
        "      if (((train_data_gen.batch_index + 1) * batch_size) > len(train_data_gen.filenames)) :\n",
        "        train_data_gen.batch_index = 0;\n",
        "      imgs, _ = next(train_data_gen) \n",
        "#       print(train_data_gen.batch_index);\n",
        "#       print(len(train_data_gen.filenames));\n",
        "#       imgs = imgs[0]\n",
        "\n",
        "      noise = np.random.normal(0, 1, (batch_size, latent_dim))\n",
        "\n",
        "      # Generate a batch of new images\n",
        "      gen_imgs = generator.predict(noise)\n",
        "\n",
        "\n",
        "      # Train the discriminator\n",
        "      d_loss_real = discriminator.train_on_batch(imgs, valid)\n",
        "      d_loss_fake = discriminator.train_on_batch(gen_imgs, fake)\n",
        "      d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
        "\n",
        "      # ---------------------\n",
        "      #  Train Generator\n",
        "      # ---------------------\n",
        "\n",
        "      noise = np.random.normal(0, 1, (batch_size, latent_dim))\n",
        "\n",
        "      # Train the generator (to have the discriminator label samples as valid)\n",
        "      g_loss = combined.train_on_batch(noise, valid)\n",
        "      # Plot the progress\n",
        "      print (\"%d [D loss: %f, acc.: %.2f%%] [G loss: %f]\" % (epoch, d_loss[0], 100*d_loss[1], g_loss))\n",
        "\n",
        "    # If at save interval => save generated image samples\n",
        "#       if epoch % sample_interval == 0:\n",
        "#           sample_images(epoch)\n",
        "\n",
        "def sample_images(epoch):\n",
        "    print('adasdasd');\n",
        "    r, c = 5, 5\n",
        "#     noise = np.random.normal(0, 1, (r * c, latent_dim))\n",
        "    noise = tf.random.normal([1, latent_dim])\n",
        "    gen_imgs = generator.predict(noise)\n",
        "    plt.imshow(gen_imgs[0, :, :, 0])\n",
        "    plt.close()\n",
        "\n",
        "    # Rescale images 0 - 1\n",
        "#     gen_imgs = 0.5 * gen_imgs + 0.5\n",
        "\n",
        "#     fig, axs = plt.subplots(r, c)\n",
        "#     cnt = 0\n",
        "#     for i in range(r):\n",
        "#         for j in range(c):\n",
        "#             axs[i,j].imshow(gen_imgs[cnt, :,:,0], cmap='gray')\n",
        "#             axs[i,j].axis('off')\n",
        "#             cnt += 1\n",
        "#     plt.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ajS1jJcKUu8J",
        "colab_type": "code",
        "outputId": "e8ecee7c-35cc-414f-c214-c9eb03162a2c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 8739
        }
      },
      "source": [
        "train(epochs=500, batch_size=BATCH_SIZE, sample_interval=10)"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/PIL/Image.py:914: UserWarning: Palette images with Transparency   expressed in bytes should be converted to RGBA images\n",
            "  'to RGBA images')\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "0 [D loss: 0.193060, acc.: 98.44%] [G loss: 1.394546]\n",
            "1 [D loss: 0.188617, acc.: 98.44%] [G loss: 1.534960]\n",
            "2 [D loss: 0.181363, acc.: 97.66%] [G loss: 1.643794]\n",
            "3 [D loss: 0.179421, acc.: 97.66%] [G loss: 1.796298]\n",
            "4 [D loss: 0.136645, acc.: 100.00%] [G loss: 2.063348]\n",
            "5 [D loss: 0.139016, acc.: 97.66%] [G loss: 2.089296]\n",
            "6 [D loss: 0.110410, acc.: 99.22%] [G loss: 2.205603]\n",
            "7 [D loss: 0.124116, acc.: 96.88%] [G loss: 2.378851]\n",
            "8 [D loss: 0.080380, acc.: 100.00%] [G loss: 2.475551]\n",
            "9 [D loss: 0.105778, acc.: 96.88%] [G loss: 2.596836]\n",
            "10 [D loss: 0.079090, acc.: 99.22%] [G loss: 2.630761]\n",
            "11 [D loss: 0.104242, acc.: 97.66%] [G loss: 2.666892]\n",
            "12 [D loss: 0.100795, acc.: 98.44%] [G loss: 2.725355]\n",
            "13 [D loss: 0.075659, acc.: 100.00%] [G loss: 2.681191]\n",
            "14 [D loss: 0.070504, acc.: 98.44%] [G loss: 2.785268]\n",
            "15 [D loss: 0.072887, acc.: 100.00%] [G loss: 2.860280]\n",
            "16 [D loss: 0.087376, acc.: 98.44%] [G loss: 2.908003]\n",
            "17 [D loss: 0.061840, acc.: 100.00%] [G loss: 2.996958]\n",
            "18 [D loss: 0.045575, acc.: 100.00%] [G loss: 3.180880]\n",
            "19 [D loss: 0.055867, acc.: 99.22%] [G loss: 3.238590]\n",
            "20 [D loss: 0.062456, acc.: 98.44%] [G loss: 3.215913]\n",
            "21 [D loss: 0.066248, acc.: 99.22%] [G loss: 3.159730]\n",
            "22 [D loss: 0.066026, acc.: 97.66%] [G loss: 3.209707]\n",
            "23 [D loss: 0.064233, acc.: 98.44%] [G loss: 3.258880]\n",
            "24 [D loss: 0.047603, acc.: 99.22%] [G loss: 3.255651]\n",
            "25 [D loss: 0.056697, acc.: 99.22%] [G loss: 3.310030]\n",
            "26 [D loss: 0.044233, acc.: 100.00%] [G loss: 3.321148]\n",
            "27 [D loss: 0.062575, acc.: 97.66%] [G loss: 3.304865]\n",
            "28 [D loss: 0.040444, acc.: 100.00%] [G loss: 3.332483]\n",
            "29 [D loss: 0.066636, acc.: 98.44%] [G loss: 3.386311]\n",
            "30 [D loss: 0.064874, acc.: 99.22%] [G loss: 3.360612]\n",
            "31 [D loss: 0.045944, acc.: 98.44%] [G loss: 3.488806]\n",
            "32 [D loss: 0.043578, acc.: 99.22%] [G loss: 3.597456]\n",
            "33 [D loss: 0.068211, acc.: 97.66%] [G loss: 3.611795]\n",
            "34 [D loss: 0.043576, acc.: 99.22%] [G loss: 3.623727]\n",
            "35 [D loss: 0.031987, acc.: 100.00%] [G loss: 3.594454]\n",
            "36 [D loss: 0.050599, acc.: 100.00%] [G loss: 3.624958]\n",
            "37 [D loss: 0.049263, acc.: 100.00%] [G loss: 3.693604]\n",
            "38 [D loss: 0.048456, acc.: 99.22%] [G loss: 3.707636]\n",
            "39 [D loss: 0.052180, acc.: 99.22%] [G loss: 3.741885]\n",
            "40 [D loss: 0.073589, acc.: 96.09%] [G loss: 3.730810]\n",
            "41 [D loss: 0.031416, acc.: 100.00%] [G loss: 3.805558]\n",
            "42 [D loss: 0.051953, acc.: 100.00%] [G loss: 3.786512]\n",
            "43 [D loss: 0.031861, acc.: 100.00%] [G loss: 3.784308]\n",
            "44 [D loss: 0.048714, acc.: 99.22%] [G loss: 3.709584]\n",
            "45 [D loss: 0.066254, acc.: 98.44%] [G loss: 3.854299]\n",
            "46 [D loss: 0.062080, acc.: 97.66%] [G loss: 3.787353]\n",
            "47 [D loss: 0.047163, acc.: 99.22%] [G loss: 3.908901]\n",
            "48 [D loss: 0.044549, acc.: 100.00%] [G loss: 3.965729]\n",
            "49 [D loss: 0.064748, acc.: 98.44%] [G loss: 3.867079]\n",
            "50 [D loss: 0.069613, acc.: 99.22%] [G loss: 3.846309]\n",
            "51 [D loss: 0.040112, acc.: 100.00%] [G loss: 3.913206]\n",
            "52 [D loss: 0.048104, acc.: 100.00%] [G loss: 3.950049]\n",
            "53 [D loss: 0.045802, acc.: 98.44%] [G loss: 3.984913]\n",
            "54 [D loss: 0.062255, acc.: 97.66%] [G loss: 4.072726]\n",
            "55 [D loss: 0.082764, acc.: 96.88%] [G loss: 4.022515]\n",
            "56 [D loss: 0.051483, acc.: 100.00%] [G loss: 4.008991]\n",
            "57 [D loss: 0.059744, acc.: 98.44%] [G loss: 4.036148]\n",
            "58 [D loss: 0.068421, acc.: 98.44%] [G loss: 4.148083]\n",
            "59 [D loss: 0.039703, acc.: 100.00%] [G loss: 4.014149]\n",
            "60 [D loss: 0.074179, acc.: 97.66%] [G loss: 4.088737]\n",
            "61 [D loss: 0.032461, acc.: 100.00%] [G loss: 4.222327]\n",
            "62 [D loss: 0.072366, acc.: 98.44%] [G loss: 4.463941]\n",
            "63 [D loss: 0.043465, acc.: 100.00%] [G loss: 4.341496]\n",
            "64 [D loss: 0.109887, acc.: 96.09%] [G loss: 4.094817]\n",
            "65 [D loss: 0.079255, acc.: 98.44%] [G loss: 4.411815]\n",
            "66 [D loss: 0.106420, acc.: 96.88%] [G loss: 4.532448]\n",
            "67 [D loss: 0.074741, acc.: 97.66%] [G loss: 4.242760]\n",
            "68 [D loss: 0.177351, acc.: 92.97%] [G loss: 5.374312]\n",
            "69 [D loss: 0.194156, acc.: 92.19%] [G loss: 4.222302]\n",
            "70 [D loss: 0.154753, acc.: 92.97%] [G loss: 4.769114]\n",
            "71 [D loss: 0.089986, acc.: 97.66%] [G loss: 4.566111]\n",
            "72 [D loss: 0.167545, acc.: 93.75%] [G loss: 4.201546]\n",
            "73 [D loss: 0.091135, acc.: 96.88%] [G loss: 4.252925]\n",
            "74 [D loss: 0.128004, acc.: 95.31%] [G loss: 3.907343]\n",
            "75 [D loss: 0.118312, acc.: 96.09%] [G loss: 4.074013]\n",
            "76 [D loss: 0.075279, acc.: 98.44%] [G loss: 4.623041]\n",
            "77 [D loss: 0.265987, acc.: 88.28%] [G loss: 6.048727]\n",
            "78 [D loss: 0.727123, acc.: 71.09%] [G loss: 4.177947]\n",
            "79 [D loss: 0.314396, acc.: 84.38%] [G loss: 4.651314]\n",
            "80 [D loss: 0.205509, acc.: 92.19%] [G loss: 5.325356]\n",
            "81 [D loss: 0.063368, acc.: 98.44%] [G loss: 5.111893]\n",
            "82 [D loss: 0.068962, acc.: 98.44%] [G loss: 4.749181]\n",
            "83 [D loss: 0.084791, acc.: 97.66%] [G loss: 4.473596]\n",
            "84 [D loss: 0.152210, acc.: 91.41%] [G loss: 4.346462]\n",
            "85 [D loss: 0.199527, acc.: 91.41%] [G loss: 3.970536]\n",
            "86 [D loss: 0.157348, acc.: 94.53%] [G loss: 3.984314]\n",
            "87 [D loss: 0.117324, acc.: 97.66%] [G loss: 4.110933]\n",
            "88 [D loss: 0.242126, acc.: 88.28%] [G loss: 3.983149]\n",
            "89 [D loss: 0.265645, acc.: 87.50%] [G loss: 3.966280]\n",
            "90 [D loss: 0.274898, acc.: 87.50%] [G loss: 3.889586]\n",
            "91 [D loss: 0.151279, acc.: 96.09%] [G loss: 3.766754]\n",
            "92 [D loss: 0.221529, acc.: 92.97%] [G loss: 4.379125]\n",
            "93 [D loss: 0.244070, acc.: 89.84%] [G loss: 4.226585]\n",
            "94 [D loss: 0.157812, acc.: 93.75%] [G loss: 4.075424]\n",
            "95 [D loss: 0.186147, acc.: 92.97%] [G loss: 3.487966]\n",
            "96 [D loss: 0.194929, acc.: 91.41%] [G loss: 3.932076]\n",
            "97 [D loss: 0.232980, acc.: 90.62%] [G loss: 4.484371]\n",
            "98 [D loss: 0.435574, acc.: 75.00%] [G loss: 3.448124]\n",
            "99 [D loss: 0.166186, acc.: 92.19%] [G loss: 4.203310]\n",
            "100 [D loss: 0.185977, acc.: 92.97%] [G loss: 3.994913]\n",
            "101 [D loss: 0.206950, acc.: 92.97%] [G loss: 3.649710]\n",
            "102 [D loss: 0.221819, acc.: 89.06%] [G loss: 4.183558]\n",
            "103 [D loss: 0.239555, acc.: 89.06%] [G loss: 4.339381]\n",
            "104 [D loss: 0.240438, acc.: 93.75%] [G loss: 4.170692]\n",
            "105 [D loss: 0.314672, acc.: 86.72%] [G loss: 5.871475]\n",
            "106 [D loss: 0.303622, acc.: 83.59%] [G loss: 4.266654]\n",
            "107 [D loss: 0.231809, acc.: 89.06%] [G loss: 5.040264]\n",
            "108 [D loss: 0.214471, acc.: 92.19%] [G loss: 4.380857]\n",
            "109 [D loss: 0.196178, acc.: 91.41%] [G loss: 4.105122]\n",
            "110 [D loss: 0.271252, acc.: 88.28%] [G loss: 3.874800]\n",
            "111 [D loss: 0.359510, acc.: 77.34%] [G loss: 4.785962]\n",
            "112 [D loss: 0.725764, acc.: 62.50%] [G loss: 3.034416]\n",
            "113 [D loss: 0.191003, acc.: 92.97%] [G loss: 4.433743]\n",
            "114 [D loss: 0.191387, acc.: 91.41%] [G loss: 3.820358]\n",
            "115 [D loss: 0.209860, acc.: 92.19%] [G loss: 3.370173]\n",
            "116 [D loss: 0.245483, acc.: 90.62%] [G loss: 4.045918]\n",
            "117 [D loss: 0.343215, acc.: 78.12%] [G loss: 4.136595]\n",
            "118 [D loss: 0.387938, acc.: 79.69%] [G loss: 3.411947]\n",
            "119 [D loss: 0.220424, acc.: 89.84%] [G loss: 3.809382]\n",
            "120 [D loss: 0.280203, acc.: 84.38%] [G loss: 3.529439]\n",
            "121 [D loss: 0.170385, acc.: 94.53%] [G loss: 3.749709]\n",
            "122 [D loss: 0.427630, acc.: 74.22%] [G loss: 3.936495]\n",
            "123 [D loss: 0.260206, acc.: 89.84%] [G loss: 3.442815]\n",
            "124 [D loss: 0.211858, acc.: 90.62%] [G loss: 3.913584]\n",
            "125 [D loss: 0.252392, acc.: 89.84%] [G loss: 3.478331]\n",
            "126 [D loss: 0.420800, acc.: 75.78%] [G loss: 3.922435]\n",
            "127 [D loss: 0.319563, acc.: 84.38%] [G loss: 4.069223]\n",
            "128 [D loss: 0.235951, acc.: 88.28%] [G loss: 3.909162]\n",
            "129 [D loss: 0.213717, acc.: 92.97%] [G loss: 4.255169]\n",
            "130 [D loss: 0.319687, acc.: 82.81%] [G loss: 3.469344]\n",
            "131 [D loss: 0.164729, acc.: 96.88%] [G loss: 4.395084]\n",
            "132 [D loss: 0.444695, acc.: 75.78%] [G loss: 3.682989]\n",
            "133 [D loss: 0.237172, acc.: 86.72%] [G loss: 4.463383]\n",
            "134 [D loss: 0.271586, acc.: 85.16%] [G loss: 3.716751]\n",
            "135 [D loss: 0.190543, acc.: 94.53%] [G loss: 3.633994]\n",
            "136 [D loss: 0.293625, acc.: 83.59%] [G loss: 4.025099]\n",
            "137 [D loss: 0.382061, acc.: 82.03%] [G loss: 3.343510]\n",
            "138 [D loss: 0.170798, acc.: 92.97%] [G loss: 4.114707]\n",
            "139 [D loss: 0.363058, acc.: 83.59%] [G loss: 3.711997]\n",
            "140 [D loss: 0.287368, acc.: 85.16%] [G loss: 3.347027]\n",
            "141 [D loss: 0.254360, acc.: 90.62%] [G loss: 3.757488]\n",
            "142 [D loss: 0.377583, acc.: 80.47%] [G loss: 4.278273]\n",
            "143 [D loss: 0.802048, acc.: 57.03%] [G loss: 2.889724]\n",
            "144 [D loss: 0.246705, acc.: 86.72%] [G loss: 3.839399]\n",
            "145 [D loss: 0.305729, acc.: 85.16%] [G loss: 3.272895]\n",
            "146 [D loss: 0.246494, acc.: 93.75%] [G loss: 3.577521]\n",
            "147 [D loss: 0.330681, acc.: 84.38%] [G loss: 3.370449]\n",
            "148 [D loss: 0.479727, acc.: 71.88%] [G loss: 3.643300]\n",
            "149 [D loss: 0.364487, acc.: 81.25%] [G loss: 3.665253]\n",
            "150 [D loss: 0.379191, acc.: 78.91%] [G loss: 5.452957]\n",
            "151 [D loss: 2.884912, acc.: 13.28%] [G loss: 1.776823]\n",
            "152 [D loss: 1.929083, acc.: 51.56%] [G loss: 0.293793]\n",
            "153 [D loss: 0.714952, acc.: 66.41%] [G loss: 1.977479]\n",
            "154 [D loss: 0.260831, acc.: 90.62%] [G loss: 2.503165]\n",
            "155 [D loss: 0.731203, acc.: 57.03%] [G loss: 1.292572]\n",
            "156 [D loss: 0.371136, acc.: 79.69%] [G loss: 1.901168]\n",
            "157 [D loss: 0.267610, acc.: 90.62%] [G loss: 2.594218]\n",
            "158 [D loss: 0.301199, acc.: 92.19%] [G loss: 2.795090]\n",
            "159 [D loss: 0.265792, acc.: 92.19%] [G loss: 2.390075]\n",
            "160 [D loss: 0.246069, acc.: 93.75%] [G loss: 2.279266]\n",
            "161 [D loss: 0.234097, acc.: 92.97%] [G loss: 2.591419]\n",
            "162 [D loss: 0.354891, acc.: 84.38%] [G loss: 2.636292]\n",
            "163 [D loss: 0.432438, acc.: 75.78%] [G loss: 2.285998]\n",
            "164 [D loss: 0.282965, acc.: 90.62%] [G loss: 2.755250]\n",
            "165 [D loss: 0.407371, acc.: 80.47%] [G loss: 2.335083]\n",
            "166 [D loss: 0.334043, acc.: 86.72%] [G loss: 2.548933]\n",
            "167 [D loss: 0.458481, acc.: 74.22%] [G loss: 2.794292]\n",
            "168 [D loss: 0.357093, acc.: 79.69%] [G loss: 2.501010]\n",
            "169 [D loss: 0.407202, acc.: 82.03%] [G loss: 2.656712]\n",
            "170 [D loss: 0.390100, acc.: 81.25%] [G loss: 2.548215]\n",
            "171 [D loss: 0.330842, acc.: 79.69%] [G loss: 3.619975]\n",
            "172 [D loss: 1.484351, acc.: 15.62%] [G loss: 0.449602]\n",
            "173 [D loss: 0.888162, acc.: 52.34%] [G loss: 1.057000]\n",
            "174 [D loss: 0.322064, acc.: 85.16%] [G loss: 2.960237]\n",
            "175 [D loss: 0.374924, acc.: 83.59%] [G loss: 2.260447]\n",
            "176 [D loss: 0.367128, acc.: 81.25%] [G loss: 2.322350]\n",
            "177 [D loss: 0.320676, acc.: 89.84%] [G loss: 2.527359]\n",
            "178 [D loss: 0.375435, acc.: 80.47%] [G loss: 2.443970]\n",
            "179 [D loss: 0.500133, acc.: 71.88%] [G loss: 2.149783]\n",
            "180 [D loss: 0.359067, acc.: 86.72%] [G loss: 2.759561]\n",
            "181 [D loss: 0.518446, acc.: 68.75%] [G loss: 2.121142]\n",
            "182 [D loss: 0.333638, acc.: 89.06%] [G loss: 2.840578]\n",
            "183 [D loss: 0.535784, acc.: 70.31%] [G loss: 1.876198]\n",
            "184 [D loss: 0.293241, acc.: 88.28%] [G loss: 2.957365]\n",
            "185 [D loss: 0.631663, acc.: 58.59%] [G loss: 1.894775]\n",
            "186 [D loss: 0.342947, acc.: 85.94%] [G loss: 3.007723]\n",
            "187 [D loss: 0.640797, acc.: 60.16%] [G loss: 2.050269]\n",
            "188 [D loss: 0.266137, acc.: 92.19%] [G loss: 2.863731]\n",
            "189 [D loss: 0.684255, acc.: 57.81%] [G loss: 1.742700]\n",
            "190 [D loss: 0.330791, acc.: 88.28%] [G loss: 2.815774]\n",
            "191 [D loss: 0.544679, acc.: 69.53%] [G loss: 2.248460]\n",
            "192 [D loss: 0.414150, acc.: 83.59%] [G loss: 2.537477]\n",
            "193 [D loss: 0.479821, acc.: 76.56%] [G loss: 2.891510]\n",
            "194 [D loss: 0.679728, acc.: 56.25%] [G loss: 1.679657]\n",
            "195 [D loss: 0.423112, acc.: 77.34%] [G loss: 3.065765]\n",
            "196 [D loss: 0.899828, acc.: 33.59%] [G loss: 1.297684]\n",
            "197 [D loss: 0.346734, acc.: 88.28%] [G loss: 2.716782]\n",
            "198 [D loss: 0.764316, acc.: 46.09%] [G loss: 1.312982]\n",
            "199 [D loss: 0.369754, acc.: 86.72%] [G loss: 2.910761]\n",
            "200 [D loss: 0.517446, acc.: 74.22%] [G loss: 1.846405]\n",
            "201 [D loss: 0.325530, acc.: 90.62%] [G loss: 2.923410]\n",
            "202 [D loss: 0.432006, acc.: 77.34%] [G loss: 2.190621]\n",
            "203 [D loss: 0.460852, acc.: 75.78%] [G loss: 2.865652]\n",
            "204 [D loss: 0.803289, acc.: 45.31%] [G loss: 1.451895]\n",
            "205 [D loss: 0.350058, acc.: 90.62%] [G loss: 2.715242]\n",
            "206 [D loss: 0.752311, acc.: 49.22%] [G loss: 1.667760]\n",
            "207 [D loss: 0.421849, acc.: 81.25%] [G loss: 2.969782]\n",
            "208 [D loss: 1.362080, acc.: 18.75%] [G loss: 0.564484]\n",
            "209 [D loss: 0.661851, acc.: 51.56%] [G loss: 1.510649]\n",
            "210 [D loss: 0.559483, acc.: 73.44%] [G loss: 1.775134]\n",
            "211 [D loss: 0.591343, acc.: 64.84%] [G loss: 1.578156]\n",
            "212 [D loss: 0.480140, acc.: 76.56%] [G loss: 2.307690]\n",
            "213 [D loss: 0.635057, acc.: 59.38%] [G loss: 1.333508]\n",
            "214 [D loss: 0.481832, acc.: 74.22%] [G loss: 2.353993]\n",
            "215 [D loss: 0.761943, acc.: 44.53%] [G loss: 1.413743]\n",
            "216 [D loss: 0.387589, acc.: 88.28%] [G loss: 2.296273]\n",
            "217 [D loss: 0.934258, acc.: 28.12%] [G loss: 0.908084]\n",
            "218 [D loss: 0.489227, acc.: 67.97%] [G loss: 2.026550]\n",
            "219 [D loss: 0.782976, acc.: 43.75%] [G loss: 1.444913]\n",
            "220 [D loss: 0.475116, acc.: 80.47%] [G loss: 2.075317]\n",
            "221 [D loss: 0.767973, acc.: 39.06%] [G loss: 1.195115]\n",
            "222 [D loss: 0.474898, acc.: 75.00%] [G loss: 2.208518]\n",
            "223 [D loss: 0.902241, acc.: 34.38%] [G loss: 0.856737]\n",
            "224 [D loss: 0.557592, acc.: 60.94%] [G loss: 1.860520]\n",
            "225 [D loss: 0.782741, acc.: 42.97%] [G loss: 1.237707]\n",
            "226 [D loss: 0.557022, acc.: 65.62%] [G loss: 2.038420]\n",
            "227 [D loss: 0.844899, acc.: 37.50%] [G loss: 0.867145]\n",
            "228 [D loss: 0.475227, acc.: 71.09%] [G loss: 1.670743]\n",
            "229 [D loss: 0.713479, acc.: 51.56%] [G loss: 1.263478]\n",
            "230 [D loss: 0.520342, acc.: 76.56%] [G loss: 1.883262]\n",
            "231 [D loss: 0.632671, acc.: 56.25%] [G loss: 1.430965]\n",
            "232 [D loss: 0.499376, acc.: 75.00%] [G loss: 2.023252]\n",
            "233 [D loss: 0.836173, acc.: 36.72%] [G loss: 1.104719]\n",
            "234 [D loss: 0.479142, acc.: 75.78%] [G loss: 1.813086]\n",
            "235 [D loss: 0.780741, acc.: 42.19%] [G loss: 1.144705]\n",
            "236 [D loss: 0.561950, acc.: 64.84%] [G loss: 1.818342]\n",
            "237 [D loss: 0.880398, acc.: 33.59%] [G loss: 0.929661]\n",
            "238 [D loss: 0.541839, acc.: 64.84%] [G loss: 1.769584]\n",
            "239 [D loss: 1.080842, acc.: 13.28%] [G loss: 0.685148]\n",
            "240 [D loss: 0.608125, acc.: 53.91%] [G loss: 1.193522]\n",
            "241 [D loss: 0.814955, acc.: 35.94%] [G loss: 1.004610]\n",
            "242 [D loss: 0.589948, acc.: 65.62%] [G loss: 1.328979]\n",
            "243 [D loss: 0.734225, acc.: 42.97%] [G loss: 0.849946]\n",
            "244 [D loss: 0.559457, acc.: 60.94%] [G loss: 1.205365]\n",
            "245 [D loss: 0.706942, acc.: 46.09%] [G loss: 1.176508]\n",
            "246 [D loss: 0.649336, acc.: 57.81%] [G loss: 1.355922]\n",
            "247 [D loss: 0.774204, acc.: 39.06%] [G loss: 0.966252]\n",
            "248 [D loss: 0.595191, acc.: 59.38%] [G loss: 1.349602]\n",
            "249 [D loss: 0.737492, acc.: 43.75%] [G loss: 1.026562]\n",
            "250 [D loss: 0.584154, acc.: 64.84%] [G loss: 1.347866]\n",
            "251 [D loss: 0.675529, acc.: 51.56%] [G loss: 1.443638]\n",
            "252 [D loss: 0.690401, acc.: 53.12%] [G loss: 1.005677]\n",
            "253 [D loss: 0.649141, acc.: 53.12%] [G loss: 1.231756]\n",
            "254 [D loss: 0.679567, acc.: 51.56%] [G loss: 1.234127]\n",
            "255 [D loss: 0.684241, acc.: 54.69%] [G loss: 1.309382]\n",
            "256 [D loss: 0.749781, acc.: 37.50%] [G loss: 1.078035]\n",
            "257 [D loss: 0.684287, acc.: 57.03%] [G loss: 1.169266]\n",
            "258 [D loss: 0.680338, acc.: 53.12%] [G loss: 1.123737]\n",
            "259 [D loss: 0.692408, acc.: 50.78%] [G loss: 1.025345]\n",
            "260 [D loss: 0.661394, acc.: 49.22%] [G loss: 1.222944]\n",
            "261 [D loss: 0.708471, acc.: 44.53%] [G loss: 1.121468]\n",
            "262 [D loss: 0.651353, acc.: 53.12%] [G loss: 1.227698]\n",
            "263 [D loss: 0.731308, acc.: 50.00%] [G loss: 1.169742]\n",
            "264 [D loss: 0.704280, acc.: 46.09%] [G loss: 1.086901]\n",
            "265 [D loss: 0.688866, acc.: 49.22%] [G loss: 1.141325]\n",
            "266 [D loss: 0.730946, acc.: 44.53%] [G loss: 1.042231]\n",
            "267 [D loss: 0.725390, acc.: 46.88%] [G loss: 1.195735]\n",
            "268 [D loss: 0.730943, acc.: 44.53%] [G loss: 1.055736]\n",
            "269 [D loss: 0.673900, acc.: 53.91%] [G loss: 1.107687]\n",
            "270 [D loss: 0.761026, acc.: 38.28%] [G loss: 1.061564]\n",
            "271 [D loss: 0.734666, acc.: 41.41%] [G loss: 0.942039]\n",
            "272 [D loss: 0.672543, acc.: 50.00%] [G loss: 1.006676]\n",
            "273 [D loss: 0.701440, acc.: 48.44%] [G loss: 1.015778]\n",
            "274 [D loss: 0.722338, acc.: 43.75%] [G loss: 0.963053]\n",
            "275 [D loss: 0.630037, acc.: 59.38%] [G loss: 1.090140]\n",
            "276 [D loss: 0.701429, acc.: 51.56%] [G loss: 1.045418]\n",
            "277 [D loss: 0.661198, acc.: 54.69%] [G loss: 1.177906]\n",
            "278 [D loss: 0.681572, acc.: 53.91%] [G loss: 1.029894]\n",
            "279 [D loss: 0.675912, acc.: 50.78%] [G loss: 0.971601]\n",
            "280 [D loss: 0.694288, acc.: 52.34%] [G loss: 1.181035]\n",
            "281 [D loss: 0.686375, acc.: 54.69%] [G loss: 1.083384]\n",
            "282 [D loss: 0.736360, acc.: 43.75%] [G loss: 0.857702]\n",
            "283 [D loss: 0.682065, acc.: 51.56%] [G loss: 1.043949]\n",
            "284 [D loss: 0.689662, acc.: 53.12%] [G loss: 0.990994]\n",
            "285 [D loss: 0.694546, acc.: 47.66%] [G loss: 0.956233]\n",
            "286 [D loss: 0.692885, acc.: 47.66%] [G loss: 1.081514]\n",
            "287 [D loss: 0.714241, acc.: 50.00%] [G loss: 1.007717]\n",
            "288 [D loss: 0.706420, acc.: 45.31%] [G loss: 1.130523]\n",
            "289 [D loss: 0.707085, acc.: 49.22%] [G loss: 1.007796]\n",
            "290 [D loss: 0.683913, acc.: 55.47%] [G loss: 1.055639]\n",
            "291 [D loss: 0.721108, acc.: 42.19%] [G loss: 1.062085]\n",
            "292 [D loss: 0.724665, acc.: 48.44%] [G loss: 0.977360]\n",
            "293 [D loss: 0.741141, acc.: 37.50%] [G loss: 1.024424]\n",
            "294 [D loss: 0.695499, acc.: 52.34%] [G loss: 0.920218]\n",
            "295 [D loss: 0.702821, acc.: 53.91%] [G loss: 1.078614]\n",
            "296 [D loss: 0.746876, acc.: 42.97%] [G loss: 0.900885]\n",
            "297 [D loss: 0.660327, acc.: 52.34%] [G loss: 0.919525]\n",
            "298 [D loss: 0.699726, acc.: 49.22%] [G loss: 1.009178]\n",
            "299 [D loss: 0.724555, acc.: 46.88%] [G loss: 0.965518]\n",
            "300 [D loss: 0.718031, acc.: 48.44%] [G loss: 0.920304]\n",
            "301 [D loss: 0.698784, acc.: 46.09%] [G loss: 0.923827]\n",
            "302 [D loss: 0.683900, acc.: 54.69%] [G loss: 0.868098]\n",
            "303 [D loss: 0.688108, acc.: 52.34%] [G loss: 1.002968]\n",
            "304 [D loss: 0.714711, acc.: 50.78%] [G loss: 0.913074]\n",
            "305 [D loss: 0.682270, acc.: 48.44%] [G loss: 0.928910]\n",
            "306 [D loss: 0.726001, acc.: 43.75%] [G loss: 0.979561]\n",
            "307 [D loss: 0.701683, acc.: 46.88%] [G loss: 0.952134]\n",
            "308 [D loss: 0.682602, acc.: 52.34%] [G loss: 0.925376]\n",
            "309 [D loss: 0.691742, acc.: 50.78%] [G loss: 0.894652]\n",
            "310 [D loss: 0.682405, acc.: 50.00%] [G loss: 0.833135]\n",
            "311 [D loss: 0.672627, acc.: 54.69%] [G loss: 0.913513]\n",
            "312 [D loss: 0.644114, acc.: 56.25%] [G loss: 1.005222]\n",
            "313 [D loss: 0.741815, acc.: 39.84%] [G loss: 0.918567]\n",
            "314 [D loss: 0.733171, acc.: 39.06%] [G loss: 0.941916]\n",
            "315 [D loss: 0.758678, acc.: 38.28%] [G loss: 0.891548]\n",
            "316 [D loss: 0.729993, acc.: 43.75%] [G loss: 0.856117]\n",
            "317 [D loss: 0.727888, acc.: 42.19%] [G loss: 0.853103]\n",
            "318 [D loss: 0.711096, acc.: 47.66%] [G loss: 0.893928]\n",
            "319 [D loss: 0.691828, acc.: 50.00%] [G loss: 0.831154]\n",
            "320 [D loss: 0.710411, acc.: 48.44%] [G loss: 0.794181]\n",
            "321 [D loss: 0.645410, acc.: 57.03%] [G loss: 0.945487]\n",
            "322 [D loss: 0.749164, acc.: 40.62%] [G loss: 0.793208]\n",
            "323 [D loss: 0.716488, acc.: 46.88%] [G loss: 0.807653]\n",
            "324 [D loss: 0.707851, acc.: 46.88%] [G loss: 0.869280]\n",
            "325 [D loss: 0.738725, acc.: 40.62%] [G loss: 0.921579]\n",
            "326 [D loss: 0.752858, acc.: 35.94%] [G loss: 0.906413]\n",
            "327 [D loss: 0.726683, acc.: 39.84%] [G loss: 0.886767]\n",
            "328 [D loss: 0.746911, acc.: 35.94%] [G loss: 0.843844]\n",
            "329 [D loss: 0.731017, acc.: 39.06%] [G loss: 0.841001]\n",
            "330 [D loss: 0.714598, acc.: 39.06%] [G loss: 0.814986]\n",
            "331 [D loss: 0.700955, acc.: 45.31%] [G loss: 0.873763]\n",
            "332 [D loss: 0.707534, acc.: 45.31%] [G loss: 0.836423]\n",
            "333 [D loss: 0.751869, acc.: 32.03%] [G loss: 0.755479]\n",
            "334 [D loss: 0.707853, acc.: 42.97%] [G loss: 0.780266]\n",
            "335 [D loss: 0.692741, acc.: 45.31%] [G loss: 0.802070]\n",
            "336 [D loss: 0.676867, acc.: 49.22%] [G loss: 0.885292]\n",
            "337 [D loss: 0.761771, acc.: 29.69%] [G loss: 0.818238]\n",
            "338 [D loss: 0.711898, acc.: 41.41%] [G loss: 0.844769]\n",
            "339 [D loss: 0.728171, acc.: 36.72%] [G loss: 0.779602]\n",
            "340 [D loss: 0.687772, acc.: 46.09%] [G loss: 0.768488]\n",
            "341 [D loss: 0.694090, acc.: 46.88%] [G loss: 0.745355]\n",
            "342 [D loss: 0.707768, acc.: 42.19%] [G loss: 0.775625]\n",
            "343 [D loss: 0.723891, acc.: 39.06%] [G loss: 0.832617]\n",
            "344 [D loss: 0.705550, acc.: 44.53%] [G loss: 0.798976]\n",
            "345 [D loss: 0.698618, acc.: 42.97%] [G loss: 0.772424]\n",
            "346 [D loss: 0.718179, acc.: 39.06%] [G loss: 0.809808]\n",
            "347 [D loss: 0.687405, acc.: 48.44%] [G loss: 0.853239]\n",
            "348 [D loss: 0.743947, acc.: 31.25%] [G loss: 0.848252]\n",
            "349 [D loss: 0.721449, acc.: 40.62%] [G loss: 0.772000]\n",
            "350 [D loss: 0.707801, acc.: 43.75%] [G loss: 0.767141]\n",
            "351 [D loss: 0.699936, acc.: 46.09%] [G loss: 0.807944]\n",
            "352 [D loss: 0.725929, acc.: 34.38%] [G loss: 0.820358]\n",
            "353 [D loss: 0.737924, acc.: 35.94%] [G loss: 0.815008]\n",
            "354 [D loss: 0.740377, acc.: 32.81%] [G loss: 0.776143]\n",
            "355 [D loss: 0.721302, acc.: 45.31%] [G loss: 0.754811]\n",
            "356 [D loss: 0.724044, acc.: 35.94%] [G loss: 0.761020]\n",
            "357 [D loss: 0.688316, acc.: 53.12%] [G loss: 0.739739]\n",
            "358 [D loss: 0.717963, acc.: 35.94%] [G loss: 0.797424]\n",
            "359 [D loss: 0.693174, acc.: 50.78%] [G loss: 0.750707]\n",
            "360 [D loss: 0.725965, acc.: 39.84%] [G loss: 0.763430]\n",
            "361 [D loss: 0.708167, acc.: 42.97%] [G loss: 0.737530]\n",
            "362 [D loss: 0.689792, acc.: 44.53%] [G loss: 0.739811]\n",
            "363 [D loss: 0.703493, acc.: 42.97%] [G loss: 0.768294]\n",
            "364 [D loss: 0.706108, acc.: 46.88%] [G loss: 0.775516]\n",
            "365 [D loss: 0.719899, acc.: 43.75%] [G loss: 0.745819]\n",
            "366 [D loss: 0.697653, acc.: 42.19%] [G loss: 0.741455]\n",
            "367 [D loss: 0.697244, acc.: 45.31%] [G loss: 0.740486]\n",
            "368 [D loss: 0.689592, acc.: 47.66%] [G loss: 0.754237]\n",
            "369 [D loss: 0.709377, acc.: 42.19%] [G loss: 0.772964]\n",
            "370 [D loss: 0.691953, acc.: 50.78%] [G loss: 0.750267]\n",
            "371 [D loss: 0.715866, acc.: 42.97%] [G loss: 0.749056]\n",
            "372 [D loss: 0.693466, acc.: 46.88%] [G loss: 0.776408]\n",
            "373 [D loss: 0.681069, acc.: 49.22%] [G loss: 0.781457]\n",
            "374 [D loss: 0.706362, acc.: 44.53%] [G loss: 0.776387]\n",
            "375 [D loss: 0.717996, acc.: 39.06%] [G loss: 0.797578]\n",
            "376 [D loss: 0.727374, acc.: 42.19%] [G loss: 0.760963]\n",
            "377 [D loss: 0.716036, acc.: 43.75%] [G loss: 0.734809]\n",
            "378 [D loss: 0.687505, acc.: 46.88%] [G loss: 0.774896]\n",
            "379 [D loss: 0.680759, acc.: 53.12%] [G loss: 0.754463]\n",
            "380 [D loss: 0.703253, acc.: 45.31%] [G loss: 0.736287]\n",
            "381 [D loss: 0.688288, acc.: 48.44%] [G loss: 0.750270]\n",
            "382 [D loss: 0.650056, acc.: 53.91%] [G loss: 0.771399]\n",
            "383 [D loss: 0.710251, acc.: 39.06%] [G loss: 0.790998]\n",
            "384 [D loss: 0.684406, acc.: 49.22%] [G loss: 0.788115]\n",
            "385 [D loss: 0.725397, acc.: 41.41%] [G loss: 0.746250]\n",
            "386 [D loss: 0.715275, acc.: 42.19%] [G loss: 0.736996]\n",
            "387 [D loss: 0.704433, acc.: 39.84%] [G loss: 0.765316]\n",
            "388 [D loss: 0.710212, acc.: 42.97%] [G loss: 0.781194]\n",
            "389 [D loss: 0.707909, acc.: 44.53%] [G loss: 0.758607]\n",
            "390 [D loss: 0.734428, acc.: 36.72%] [G loss: 0.736917]\n",
            "391 [D loss: 0.703046, acc.: 42.97%] [G loss: 0.760604]\n",
            "392 [D loss: 0.710576, acc.: 43.75%] [G loss: 0.804794]\n",
            "393 [D loss: 0.709866, acc.: 47.66%] [G loss: 0.821540]\n",
            "394 [D loss: 0.725329, acc.: 38.28%] [G loss: 0.781706]\n",
            "395 [D loss: 0.726918, acc.: 39.84%] [G loss: 0.762843]\n",
            "396 [D loss: 0.715982, acc.: 44.53%] [G loss: 0.741984]\n",
            "397 [D loss: 0.711461, acc.: 38.28%] [G loss: 0.766793]\n",
            "398 [D loss: 0.714224, acc.: 42.97%] [G loss: 0.789926]\n",
            "399 [D loss: 0.709530, acc.: 42.19%] [G loss: 0.740377]\n",
            "400 [D loss: 0.702543, acc.: 42.19%] [G loss: 0.731846]\n",
            "401 [D loss: 0.694472, acc.: 48.44%] [G loss: 0.713015]\n",
            "402 [D loss: 0.720685, acc.: 35.94%] [G loss: 0.758707]\n",
            "403 [D loss: 0.694582, acc.: 43.75%] [G loss: 0.758308]\n",
            "404 [D loss: 0.729406, acc.: 36.72%] [G loss: 0.731540]\n",
            "405 [D loss: 0.703686, acc.: 40.62%] [G loss: 0.743023]\n",
            "406 [D loss: 0.709886, acc.: 35.16%] [G loss: 0.752872]\n",
            "407 [D loss: 0.722184, acc.: 42.19%] [G loss: 0.759212]\n",
            "408 [D loss: 0.708222, acc.: 42.19%] [G loss: 0.741839]\n",
            "409 [D loss: 0.718662, acc.: 39.84%] [G loss: 0.715902]\n",
            "410 [D loss: 0.693424, acc.: 46.88%] [G loss: 0.749157]\n",
            "411 [D loss: 0.683177, acc.: 50.00%] [G loss: 0.757753]\n",
            "412 [D loss: 0.732168, acc.: 37.50%] [G loss: 0.736805]\n",
            "413 [D loss: 0.724040, acc.: 38.28%] [G loss: 0.761042]\n",
            "414 [D loss: 0.720291, acc.: 38.28%] [G loss: 0.759909]\n",
            "415 [D loss: 0.718866, acc.: 42.97%] [G loss: 0.774225]\n",
            "416 [D loss: 0.737970, acc.: 35.16%] [G loss: 0.751139]\n",
            "417 [D loss: 0.720098, acc.: 37.50%] [G loss: 0.735058]\n",
            "418 [D loss: 0.702660, acc.: 39.84%] [G loss: 0.770317]\n",
            "419 [D loss: 0.735392, acc.: 31.25%] [G loss: 0.741095]\n",
            "420 [D loss: 0.704120, acc.: 43.75%] [G loss: 0.748293]\n",
            "421 [D loss: 0.710497, acc.: 44.53%] [G loss: 0.733486]\n",
            "422 [D loss: 0.704539, acc.: 41.41%] [G loss: 0.738228]\n",
            "423 [D loss: 0.722826, acc.: 38.28%] [G loss: 0.759303]\n",
            "424 [D loss: 0.714344, acc.: 42.97%] [G loss: 0.759419]\n",
            "425 [D loss: 0.723489, acc.: 38.28%] [G loss: 0.736272]\n",
            "426 [D loss: 0.722445, acc.: 35.16%] [G loss: 0.728142]\n",
            "427 [D loss: 0.721176, acc.: 37.50%] [G loss: 0.736453]\n",
            "428 [D loss: 0.703031, acc.: 42.97%] [G loss: 0.717124]\n",
            "429 [D loss: 0.729663, acc.: 38.28%] [G loss: 0.721608]\n",
            "430 [D loss: 0.719250, acc.: 38.28%] [G loss: 0.731511]\n",
            "431 [D loss: 0.735387, acc.: 35.16%] [G loss: 0.722880]\n",
            "432 [D loss: 0.712769, acc.: 46.09%] [G loss: 0.728781]\n",
            "433 [D loss: 0.719914, acc.: 38.28%] [G loss: 0.711651]\n",
            "434 [D loss: 0.713791, acc.: 44.53%] [G loss: 0.726538]\n",
            "435 [D loss: 0.722360, acc.: 38.28%] [G loss: 0.766370]\n",
            "436 [D loss: 0.729415, acc.: 41.41%] [G loss: 0.762059]\n",
            "437 [D loss: 0.711475, acc.: 45.31%] [G loss: 0.732307]\n",
            "438 [D loss: 0.733653, acc.: 39.06%] [G loss: 0.733869]\n",
            "439 [D loss: 0.692285, acc.: 49.22%] [G loss: 0.738324]\n",
            "440 [D loss: 0.700660, acc.: 46.88%] [G loss: 0.765998]\n",
            "441 [D loss: 0.727318, acc.: 35.94%] [G loss: 0.755463]\n",
            "442 [D loss: 0.736837, acc.: 32.81%] [G loss: 0.774893]\n",
            "443 [D loss: 0.718748, acc.: 42.19%] [G loss: 0.758088]\n",
            "444 [D loss: 0.734115, acc.: 34.38%] [G loss: 0.741416]\n",
            "445 [D loss: 0.727820, acc.: 36.72%] [G loss: 0.747552]\n",
            "446 [D loss: 0.730973, acc.: 33.59%] [G loss: 0.716893]\n",
            "447 [D loss: 0.722757, acc.: 40.62%] [G loss: 0.752726]\n",
            "448 [D loss: 0.723599, acc.: 40.62%] [G loss: 0.725281]\n",
            "449 [D loss: 0.722950, acc.: 36.72%] [G loss: 0.732185]\n",
            "450 [D loss: 0.728188, acc.: 39.06%] [G loss: 0.729255]\n",
            "451 [D loss: 0.724742, acc.: 39.84%] [G loss: 0.740669]\n",
            "452 [D loss: 0.714299, acc.: 41.41%] [G loss: 0.737926]\n",
            "453 [D loss: 0.735505, acc.: 30.47%] [G loss: 0.748859]\n",
            "454 [D loss: 0.720761, acc.: 44.53%] [G loss: 0.738673]\n",
            "455 [D loss: 0.720204, acc.: 39.84%] [G loss: 0.760879]\n",
            "456 [D loss: 0.733497, acc.: 34.38%] [G loss: 0.746817]\n",
            "457 [D loss: 0.712956, acc.: 42.97%] [G loss: 0.724743]\n",
            "458 [D loss: 0.718904, acc.: 40.62%] [G loss: 0.750362]\n",
            "459 [D loss: 0.727002, acc.: 38.28%] [G loss: 0.729148]\n",
            "460 [D loss: 0.714055, acc.: 41.41%] [G loss: 0.744478]\n",
            "461 [D loss: 0.735313, acc.: 32.81%] [G loss: 0.736460]\n",
            "462 [D loss: 0.717325, acc.: 43.75%] [G loss: 0.712224]\n",
            "463 [D loss: 0.716280, acc.: 39.06%] [G loss: 0.723894]\n",
            "464 [D loss: 0.714217, acc.: 40.62%] [G loss: 0.715363]\n",
            "465 [D loss: 0.716626, acc.: 40.62%] [G loss: 0.719693]\n",
            "466 [D loss: 0.713694, acc.: 42.19%] [G loss: 0.752480]\n",
            "467 [D loss: 0.718902, acc.: 40.62%] [G loss: 0.734601]\n",
            "468 [D loss: 0.726028, acc.: 38.28%] [G loss: 0.727544]\n",
            "469 [D loss: 0.712662, acc.: 36.72%] [G loss: 0.730542]\n",
            "470 [D loss: 0.722352, acc.: 33.59%] [G loss: 0.753108]\n",
            "471 [D loss: 0.704139, acc.: 46.09%] [G loss: 0.757856]\n",
            "472 [D loss: 0.733062, acc.: 33.59%] [G loss: 0.773983]\n",
            "473 [D loss: 0.729371, acc.: 36.72%] [G loss: 0.758369]\n",
            "474 [D loss: 0.722149, acc.: 41.41%] [G loss: 0.751312]\n",
            "475 [D loss: 0.701688, acc.: 50.00%] [G loss: 0.720245]\n",
            "476 [D loss: 0.706947, acc.: 42.19%] [G loss: 0.725557]\n",
            "477 [D loss: 0.716470, acc.: 38.28%] [G loss: 0.733210]\n",
            "478 [D loss: 0.713677, acc.: 40.62%] [G loss: 0.754363]\n",
            "479 [D loss: 0.725457, acc.: 35.94%] [G loss: 0.776787]\n",
            "480 [D loss: 0.736426, acc.: 31.25%] [G loss: 0.728598]\n",
            "481 [D loss: 0.720305, acc.: 42.97%] [G loss: 0.734771]\n",
            "482 [D loss: 0.727607, acc.: 39.06%] [G loss: 0.724978]\n",
            "483 [D loss: 0.707774, acc.: 42.97%] [G loss: 0.738889]\n",
            "484 [D loss: 0.712335, acc.: 35.94%] [G loss: 0.733008]\n",
            "485 [D loss: 0.715408, acc.: 46.09%] [G loss: 0.724183]\n",
            "486 [D loss: 0.704944, acc.: 41.41%] [G loss: 0.721825]\n",
            "487 [D loss: 0.704704, acc.: 42.19%] [G loss: 0.744865]\n",
            "488 [D loss: 0.707354, acc.: 43.75%] [G loss: 0.728138]\n",
            "489 [D loss: 0.731226, acc.: 32.81%] [G loss: 0.744849]\n",
            "490 [D loss: 0.714145, acc.: 37.50%] [G loss: 0.766226]\n",
            "491 [D loss: 0.727077, acc.: 32.03%] [G loss: 0.781944]\n",
            "492 [D loss: 0.730255, acc.: 35.16%] [G loss: 0.791111]\n",
            "493 [D loss: 0.741791, acc.: 25.00%] [G loss: 0.750532]\n",
            "494 [D loss: 0.705331, acc.: 44.53%] [G loss: 0.746350]\n",
            "495 [D loss: 0.719350, acc.: 42.19%] [G loss: 0.718090]\n",
            "496 [D loss: 0.731198, acc.: 29.69%] [G loss: 0.716043]\n",
            "497 [D loss: 0.732135, acc.: 33.59%] [G loss: 0.739047]\n",
            "498 [D loss: 0.721710, acc.: 32.03%] [G loss: 0.760159]\n",
            "499 [D loss: 0.720514, acc.: 37.50%] [G loss: 0.754115]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "POfy48ScxXIr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 287
        },
        "outputId": "fd4003f2-4f59-41f0-8234-9b3a21b5afde"
      },
      "source": [
        "noise = tf.random.normal([1, latent_dim])\n",
        "gen_imgs = generator.predict(noise)\n",
        "plt.imshow(gen_imgs[0, :, :, 0])"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f3f85162c50>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 64
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nNj+Rp9j8RJ5i8xN56v8ByO+4mP6WwVkAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}